{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23dad62a",
   "metadata": {
    "is_executing": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/43669/repos/retmar/src/retailsynth/utils/dataset_viz.py:102: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(\n",
      "/Users/43669/repos/retmar/src/retailsynth/utils/dataset_viz.py:111: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import altair as alt\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from hydra import compose, initialize\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from retailsynth import REPO_ROOT_DIR\n",
    "from retailsynth.base_config import load_config_store\n",
    "from retailsynth.datasets.complete_journey.preprocess_pipeline import run_preprocess\n",
    "from retailsynth.utils.storage import clear_cwd\n",
    "from retailsynth.utils.dataset_viz import viz_main\n",
    "from retailsynth.utils.storage import load_result\n",
    "from calib_utils.plot_discounts import (\n",
    "    plot_discount_series,\n",
    "    prepare_discount_df,\n",
    "    plot_status_transition,\n",
    ")\n",
    "from calib_utils.plot_elasticity import (\n",
    "    list_elasticity_one_step_kdeplot,\n",
    "    list_elasticity_one_step_heatmap,\n",
    "    initialize_elasticity_viz_data_layer,\n",
    ")\n",
    "from calib_utils.plot_purchases import (\n",
    "    Plotter,\n",
    ")\n",
    "from calib_utils.plot_utils import (\n",
    "    make_plot_data,\n",
    "    make_exploratory_plot,\n",
    "    plot_price_history,\n",
    ")\n",
    "\n",
    "alt.renderers.enable(\"png\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b4b7b",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook includes the collection of data analysis steps used to calibrate and validate the RetailSynth data synthesizer. While developing Retailsynth, we tuned model hyperparameters manually leveraging this notebook for feedback and also used it to validate the results of fine-tuning, performed with Bayesian hyperparameter optimization as implemented in `analysis_workflow/2_synthesizer_calibration/2_parameter_sweeping.py.` \n",
    "\n",
    "## Model Overview\n",
    "RetailSynth is based on an interpretable multi-stage model that captures the sequential nature of customer decision-making. In this model, the consumer needs to make a collection of decisions, from whether to visit the retail store, which category of products to purchase, which product from the selected category to purchase, to how many units of selected products to purchase. Each step can only be triggered if the answer to the prior step is positive. Thus, the environment captures that, for a customer $u$, product $i$, category $j$ individual demand is calculated using the following conditional probability distribution:\n",
    "\n",
    "$$\n",
    "P(Q_{ui} = q,S_u,C_{uj},B_{ui}) = \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{StoreVisit} * \\text{CategoryChoice} * \\text{ProductChoice} * \\text{ProductDemand}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=P(S_u) * P(C_{uj} | S_u) * P(B_{ui}|S_u,C_{uj}) * P(Q_{ui}=q|S_u,C_{uj},B_{ui})\n",
    "$$\n",
    "\n",
    "The model is intended to capture both short-term purchase behavior and long-term customer loyalty. _For a detailed formulation of the model at each step, please refer to the paper._\n",
    "\n",
    "## Calibration and Validation Process\n",
    "To calibrate the synthesizer, we set up a simulation of 100 customers over a period of 53 weeks utilizing a product catalog that mirrors the Complete Journey dataset, encompassing approximately âˆ¼26,000 products across 300 categories. Within this notebook, we compare a collection of probability distributions and key metrics generated by the data synthesizer against real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30953d7f-857d-439c-afa8-b888bec8d6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing Pipeline \n",
    "\n",
    "To ensure reproducibility, we re-run the synthetic data generation and feature engineering workflows in this notebook. The driver function is called `viz_main` and can execute the end-to-end pipeline. The steps include:\n",
    "\n",
    "- Generation of synthetic transactions and pricing histories, or loading real-world data.\n",
    "- Feature engineering at store, category, and product levels from transaction data.\n",
    "- Saving engineered features.\n",
    "\n",
    "\n",
    "#### Run feature engineering on Complete Journey dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11911e44-88c6-4944-9fb8-6f75f016fa71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = REPO_ROOT_DIR / \"analysis_workflow/2_synthesizer_calibration/outputs\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(output_path)\n",
    "os.getcwd()\n",
    "\n",
    "with initialize(version_base=None, config_path=\"cfg\"):\n",
    "    load_config_store()\n",
    "    cfg = compose(config_name=\"real_dataset\")\n",
    "    clear_cwd(cfg.paths.processed_data)\n",
    "    viz_main(cfg)\n",
    "    REAL_DATA_PATH = cfg.paths.processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ff764-33ed-4b65-b30c-d6bd1db8e269",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79d994-1d03-4674-9330-b2cfc93d0e01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synthesizing trajectory:   0%|                                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"cfg\"):\n",
    "    load_config_store()\n",
    "    cfg = compose(config_name=\"synthetic_dataset\")\n",
    "    clear_cwd(cfg.paths.processed_data)\n",
    "    viz_main(cfg)\n",
    "    SYNTHETIC_DATA_PATH = cfg.paths.processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377be75-f7c6-4b55-ae0b-9a40948c1686",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c8801-c5bf-428e-a9f8-12945547970a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "complete_journey_result = load_result(output_path / REAL_DATA_PATH)\n",
    "synthetic_result = load_result(output_path / SYNTHETIC_DATA_PATH)\n",
    "with open(output_path / SYNTHETIC_DATA_PATH / \"synthesizer.pickle\", \"rb\") as fp:\n",
    "    synthesizer = pickle.load(fp)\n",
    "\n",
    "# run pre-process scripts to load the datasets\n",
    "with initialize(version_base=None, config_path=\"cfg\"):\n",
    "    load_config_store()\n",
    "    cfg = compose(config_name=\"real_dataset\", overrides=[\"n_customers_sampled=null\"])\n",
    "    cfg = OmegaConf.to_object(cfg)\n",
    "    customers, products, transactions = run_preprocess(cfg)\n",
    "\n",
    "weekly_aggregates = make_plot_data(transactions, products)\n",
    "\n",
    "category_catalog = (\n",
    "    products[\n",
    "        [\n",
    "            \"manufacturer_id\",\n",
    "            \"department\",\n",
    "            \"brand\",\n",
    "            \"category_desc\",\n",
    "            \"subcategory_desc\",\n",
    "            \"package_size\",\n",
    "            \"all\",\n",
    "            \"category_nbr\",\n",
    "        ]\n",
    "    ]\n",
    "    .drop_duplicates(\"category_nbr\")\n",
    "    .set_index(\"category_nbr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9b422-5471-4744-96b3-c7df361c3a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = [complete_journey_result, synthetic_result]\n",
    "labels = [\"real_data\", \"synthetic_data\"]\n",
    "colors = sns.color_palette(n_colors=len(results))\n",
    "\n",
    "plotter = Plotter(\n",
    "    results=[complete_journey_result, synthetic_result],\n",
    "    labels=[\"real_data\", \"synthetic_data\"],\n",
    "    row_size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e3a05-fad2-43d8-8a22-803ed836609f",
   "metadata": {},
   "source": [
    "### Demand analysis\n",
    "\n",
    "Since we are using the Complete Journey data for calibration, we want to understand the extent of price variation and its effect on demand in the dataset. The data is quite noisy in aggregate, so we simplify our analysis by looking at specific products within a sub-category with high sales volume and pricing variability. Let us take a look at the `economy` subcategory of the `bacon` category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c09e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CATEGORY_OF_INTEREST = \"bacon\"  # 'fluid milk products'\n",
    "SUBCATEGORY_OF_INTEREST = \"economy\"  # 'fluid milk white only'\n",
    "POLYNOMIAL_DEGREE = 1\n",
    "make_exploratory_plot(\n",
    "    weekly_aggregates,\n",
    "    category=CATEGORY_OF_INTEREST,\n",
    "    subcategory=SUBCATEGORY_OF_INTEREST,\n",
    "    plot_poly_order=POLYNOMIAL_DEGREE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a36cf9-bdf8-4e5f-b4a8-1aaeee919683",
   "metadata": {},
   "source": [
    "Our analysis confirms a classic demand response to pricing - demand decreases with increasing prices. However, we only capture a partial view as extreme discount data is absent (likely due to the retailer safeguarding their profit margins). Similar trends in other categories verify this pattern. We expect that calibrating the synthesizer to match this dataset will produce a meaningful demand curve. (We verify this in the scenario analysis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e86cdf-7548-4157-b13b-c5502a51c7f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pricing Data Analysis\n",
    "Typical retail datasets, like the Complete Journey dataset, do not include a complete price history for all products, as they are produced by legacy systems that log only transactions. We need to understand the pricing policies used in this dataset and then implement a synthetic analog in the syntheizer.  \n",
    "\n",
    "### Price Histories in Complete Journey Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2fcf4-5133-4817-89fb-a945c7e75be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "water_plot = plot_price_history(\n",
    "    weekly_aggregates,\n",
    "    product_list=[\"3208\", \"12642\", \"3210\"],\n",
    "    category=\"Flavored Water\",\n",
    "    color_scheme=\"set1\",\n",
    ")\n",
    "bacon_plot = plot_price_history(\n",
    "    weekly_aggregates,\n",
    "    product_list=[\"1020\", \"10308\", \"2567\"],\n",
    "    category=\"Bacon\",\n",
    "    color_scheme=\"set2\",\n",
    ")\n",
    "candy_plot = plot_price_history(\n",
    "    weekly_aggregates,\n",
    "    product_list=[\"1390\", \"23269\", \"2760\"],\n",
    "    category=\"Packaged Candy\",\n",
    "    color_scheme=\"magma\",\n",
    ")\n",
    "\n",
    "fig = (\n",
    "    alt.vconcat(water_plot, bacon_plot, candy_plot)\n",
    "    .resolve_scale(color=\"independent\")\n",
    "    .configure_legend(disable=True)\n",
    "    .configure_axis(labelFontSize=13, titleFontSize=13, titleFontWeight=\"normal\")\n",
    "    .configure_title(fontSize=15, fontWeight=\"bold\")\n",
    ")\n",
    "fig.save(\"price_history_real_data.png\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2f93f-8b2d-42ec-9229-15628ab54c0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Chronological pricing analysis of a few product categories highlights deliberate, non-random price-setting policies. The baseline pricing strategy appears to follow a high-low pattern, with distinct discount frequencies and depths across product types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c786bca-4a03-43b5-b961-829e73974b5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Synthesizer Pricing Policy Analysis\n",
    "\n",
    "As described in the paper, our synthesizer uses a stochastic pricing policy, based on a Hidden Markov Model. Within the synthetic dataset, we compare the price history for a product with frequent discounts and one with few discounts to ensure the model is producing diverse pricing policies for different products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282e079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transition_prob = synthesizer.transition_prob.mean\n",
    "diff = transition_prob[:, 0] - transition_prob[:, 1]\n",
    "no_discount_product = int(diff.argmin())\n",
    "always_discount_product = int(diff.argmax())\n",
    "p_index = [no_discount_product, always_discount_product]\n",
    "\n",
    "select_product_price_df = prepare_discount_df(synthesizer, p_index)\n",
    "ax = plot_discount_series(select_product_price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e0097-c315-4d87-870a-a16dae4422fc",
   "metadata": {},
   "source": [
    "We see that these products reflect very different discounting regimes, as expected. Next, we look to understand the aggregate effects of the pricing policies.\n",
    "\n",
    "The graph below shows the percentage of discount state and no-discount state across products, each week. We can see that about 80% of products are offered at the original price and 20% of products are on sale each week in the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782f2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discount_state = jnp.array(synthesizer.choice_decision_stats[\"discount_state\"])\n",
    "axes = plot_status_transition(discount_state, n_product=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3435ed-cb48-4a0e-92cd-95328c5fccd8",
   "metadata": {},
   "source": [
    "# Comparison of Synthetic and Complete Journey Datasets\n",
    "We compare here the distributions for the synthetic and Complete Journey datasets that describe the product catalog, customer purchase decisions, and aggregate business metrics.  \n",
    "\n",
    "## Product Price and Category Size Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dd873-3720-437a-b923-785a508bc886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting_dirs = [\n",
    "    (\n",
    "        plotter.plot_product_price_variation,\n",
    "        dict(binwidth=1, xlim_min=0, xlim_max=15, legend=True),\n",
    "    ),\n",
    "    (\n",
    "        plotter.plot_product_counts_per_category,\n",
    "        dict(binwidth=10, xlim_min=0, xlim_max=200),\n",
    "    ),\n",
    "]\n",
    "fig, axes = plotter.examine_datasets(\n",
    "    plotting_dirs,\n",
    "    row_size=7,\n",
    "    label_args={\"fontsize\": 12},\n",
    "    legend_args={\"fontsize\": 12, \"loc\": \"lower right\"},\n",
    "    text_args={\"fontsize\": 12},\n",
    "    text_loc=\"upper right\",\n",
    ")\n",
    "fig.savefig(\"synthetic_price_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742a362-178d-44c6-acb9-569de0e6c402",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choice Probability Distributions\n",
    "\n",
    "We calibrated the synthesizer to generate customer dynamic similar to the Complete Journey dataset. Here, we visualize the shape of choice probability distribution for each step. Our objective is to determine if our synthesizer is able to replicate the shape of the distributions, which tend to be heavy-tailed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64dde1-7edd-45f0-a480-0546a2d998d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Throughout this section, we leverage the KS complement and Range coverage metrics to assess the goodness-of-fit. For both, scores of 1 are the best and 0 the worst. We limit our discussion of the implications of these fit metrics here and refer the interested reader to the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa2354-d334-4f30-a7cf-09ce1846663a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ACQ_WEEK_THR = 3  # Threshold for when customers must be acquired by to look at time since last purchase.\n",
    "\n",
    "plotting_dirs = [\n",
    "    (\n",
    "        plotter.plot_customer_visits,\n",
    "        dict(\n",
    "            binwidth=0.03, xlim_min=0.3, xlim_max=1, acq_week=ACQ_WEEK_THR, legend=False\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        plotter.plot_category_purchase_freq_by_hierarchy,\n",
    "        dict(binwidth=0.02, xlim_min=0, xlim_max=0.3),\n",
    "    ),\n",
    "    (\n",
    "        plotter.plot_product_purchase_freq_by_hierarchy,\n",
    "        dict(binwidth=0.01, xlim_min=0, xlim_max=0.15),\n",
    "    ),\n",
    "    (plotter.plot_individual_demand, dict(binwidth=1, xlim_min=1, xlim_max=10)),\n",
    "]\n",
    "fig, axes = plotter.examine_datasets(\n",
    "    plotting_dirs,\n",
    "    row_size=7,\n",
    "    label_args={\"fontsize\": 12},\n",
    "    legend_args={\"fontsize\": 12},\n",
    "    text_args={\"fontsize\": 12},\n",
    "    text_loc=\"upper right\",\n",
    ")\n",
    "fig.savefig(\"outcome_distribution_fit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c75ca-b031-4b26-afd5-75d00bfc05ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Key Metrics\n",
    "\n",
    "The following distributions show key aggregate business metrics describing customer loyalty, category penetration, basket size, and sales volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340db2a5-1678-46a4-871a-ec84083c9381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotting_dirs = [\n",
    "    (\n",
    "        plotter.plot_time_since_last_purchase,\n",
    "        dict(binwidth=1, xlim_min=1, xlim_max=7, legend=True),\n",
    "    ),\n",
    "    (\n",
    "        plotter.plot_basket_size_category_count,\n",
    "        dict(binwidth=1, xlim_min=0, xlim_max=75),\n",
    "    ),\n",
    "    (plotter.plot_basket_size, dict(binwidth=1, xlim_min=0, xlim_max=50)),\n",
    "    (plotter.plot_product_sales, dict(binwidth=1, xlim_min=0, xlim_max=50)),\n",
    "]\n",
    "fig, axes = plotter.examine_datasets(\n",
    "    plotting_dirs,\n",
    "    row_size=7,\n",
    "    label_args={\"fontsize\": 12},\n",
    "    legend_args={\"fontsize\": 12, \"loc\": \"lower right\"},\n",
    "    text_args={\"fontsize\": 12},\n",
    "    text_loc=\"upper right\",\n",
    ")\n",
    "fig.savefig(\"key_metric_distribution_fit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97960273-c124-4f5c-a18a-767939a1ca17",
   "metadata": {},
   "source": [
    "# Price Elasticity Analysis\n",
    "## Price Elasticities across Products and Customers\n",
    "\n",
    "Our multi-stage framework allows us to derive price elasticity of one product at different levels defined in a\n",
    "closed-form manner. While simulating the customer trajectories, RetailSynth stores the price elasticity for each step internally in the\n",
    "container `elasticity_stats`. We implement a `DataLayer` object to convert the elasticity array to a more readable\n",
    "representation in the data frame format. _The analytic elasticity formulas are provided in the appendix of the paper._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354b07a-14f5-4576-ab71-e2bda38b8b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = initialize_elasticity_viz_data_layer(synthesizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb2bbc",
   "metadata": {},
   "source": [
    "The heatmap below visualizes the mean elasticity value for each customer and each product. The color of the heatmap\n",
    "represents the elasticity value, the x-axis represents the customer id and the y-axis is the product id. This graph\n",
    "shows that our synthesizer is able to provide varied price response across products and across customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cae9b3-e0fb-4c53-ad9f-e792dc8ab704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_customer = 10\n",
    "n_product = 10\n",
    "category_nbr = 2\n",
    "customer_product_elasticity = dl.get_elasticity_per_customer_per_product()\n",
    "customer_key = customer_product_elasticity.customer_key.unique()[:n_customer]\n",
    "product_nbr = customer_product_elasticity.query(\n",
    "    f\"category_nbr == {category_nbr}\"\n",
    ").product_nbr.unique()[:n_product]\n",
    "\n",
    "ax = list_elasticity_one_step_heatmap(\n",
    "    customer_product_elasticity,\n",
    "    \"overall_elasticity\",\n",
    "    \"Overall elasticity\",\n",
    "    category_nbr=category_nbr,\n",
    "    customer_list=customer_key,\n",
    "    product_list=product_nbr,\n",
    ")\n",
    "ax.set_title(\"Overall price elasticity\", fontsize=15, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Customer\", fontsize=12)\n",
    "ax.set_ylabel(\"Product\", fontsize=12)\n",
    "ax.get_figure().savefig(\"elasticity_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741c8ec-e575-48c6-912b-e867320c19fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Consumer-Specific Elasticity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb877e23-1d80-4c78-ad7f-c80defab17d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRODUCT_IDX = 0\n",
    "n_customer = -1\n",
    "plot_elasticity_df = dl.get_elasticity_per_week_per_customer_per_product(PRODUCT_IDX)\n",
    "customer_list = plot_elasticity_df.customer_key.unique()[:n_customer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda71e4",
   "metadata": {},
   "source": [
    "We generate contour plots for price elasticity of product. Each observation of elasticity is from one customer at one\n",
    "time step and the color of the contour plot represents different probability densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac97ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = list_elasticity_one_step_kdeplot(\n",
    "    plot_elasticity_df,\n",
    "    \"expected_demand\",\n",
    "    \"overall_elasticity\",\n",
    "    xlabel=\"Expected purchase quantity per customer per week\",\n",
    "    ylabel=\"Price elasticity of demand\",\n",
    "    color=sns.color_palette(\"rocket\", as_cmap=True),\n",
    "    customer_key=customer_list,\n",
    "    legend_loc=\"lower right\",\n",
    "    ylim_bottom=None,\n",
    "    ylim_top=None,\n",
    "    xlim_left=1e-42,\n",
    "    xlim_right=None,\n",
    "    log_scale=(True, False),\n",
    ")\n",
    "ax.set_title(\n",
    "    \"Customer-specific price elasticity \\n for an individual product\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(\"Expected purchase quantity per customer per week\", fontsize=12)\n",
    "ax.set_ylabel(\"Price elasticity\", fontsize=12)\n",
    "ax.get_figure().savefig(\"elasticity_kde.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affbb9db-8d1c-4984-818e-ca71340d35f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Appendix: Category Choice Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cbd13-04bb-4f48-8280-3c0598d52dfe",
   "metadata": {},
   "source": [
    "One of the key assumptions we make in our modeling framework is that each category is chosen independently. This assumption gives rise to basket sizes following a relatively narrow distribution, as opposed to a heavy-tailed distribution resulting from a more complex choice process that we observe in the Complete Journey dataset. \n",
    "\n",
    "We perform market basket analysis at the category level to see if there is a significant\n",
    "affinity for categories to be purchased together using the Apriori algorithm, a popular method used in data mining for\n",
    "extracting such association rules. \n",
    "\n",
    "Key concepts crucial to understanding the Apriori algorithm's workings include Support, Confidence, and Lift. Each of\n",
    "these measures provides different insights into the strength and reliability of discovered associations.\n",
    "\n",
    "**Support** is an indication of how frequently a category set appears in the dataset. Mathematically, it can be denoted\n",
    "as:\n",
    "\n",
    "$$ Support(X) = \\frac{Freq(X)}{N} $$\n",
    "\n",
    "where $Freq(X)$ is the number of transactions where category X appears, and $N$ is the total number of transactions. We\n",
    "can also call it category choice probability.\n",
    "\n",
    "**Confidence** is a measure of the probability that category Y is purchased given category X is purchased. It is\n",
    "expressed as:\n",
    "\n",
    "$$ Confidence(X \\rightarrow Y) = \\frac{Support(X, Y)}{Support(X)} $$\n",
    "\n",
    "where $Support(X, Y)$ is the probability of the category set containing both X and Y. Confidence is equal to the conditional\n",
    "probability $P(X|Y)$.\n",
    "\n",
    "**Lift** indicates the strength of a rule over the random occurrence of X and Y. It is a measure of the likelihood that\n",
    "category Y is bought when category X is bought while considering the popularity of Y.\n",
    "\n",
    "- If Lift is much larger than 1, it means that category Y is likely to be bought if category X is bought.\n",
    "- If Lift is small and the support of X is big, it means that category Y is unlikely to be bought if category X is\n",
    "  bought.\n",
    "\n",
    "Mathematically, it can be represented as:\n",
    "\n",
    "$$ Lift(X \\rightarrow Y) = \\frac{Confidence(X \\rightarrow Y)}{Support(Y)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b8ba6-498e-41d6-bf2d-846801ebc8da",
   "metadata": {},
   "source": [
    "## Category choice in Complete Journey Dataset\n",
    "Let's extract product association metrics from the real datase for each tuple of categories of length two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb06b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare one-hot-encoding vector for each basket\n",
    "real_category_choice = complete_journey_result[\"category_choice\"].reset_index()[\n",
    "    [\"category_nbr\", \"week\", \"customer_key\", \"category_choice\"]\n",
    "]\n",
    "real_category_choice = real_category_choice.query(\"category_choice == 1\")\n",
    "real_category_choice = real_category_choice.groupby([\"week\", \"customer_key\"])[\n",
    "    \"category_nbr\"\n",
    "].apply(list)\n",
    "encoder = TransactionEncoder()\n",
    "te_ary = encoder.fit(real_category_choice).transform(real_category_choice)\n",
    "df = pd.DataFrame(te_ary, columns=encoder.columns_)\n",
    "\n",
    "# Building the model\n",
    "frq_items = apriori(df, min_support=0.01, max_len=2, use_colnames=True)\n",
    "\n",
    "# Collecting the inferred rules in a dataframe\n",
    "real_rules = association_rules(frq_items, metric=\"lift\", min_threshold=0)\n",
    "\n",
    "# Plot the category lift distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax = sns.histplot(real_rules.lift, stat=\"probability\")\n",
    "ax.set_title(\"Category-level lift distribution\", weight=\"bold\", fontsize=15)\n",
    "ax.set_xlabel(\"Lift\", fontsize=13)\n",
    "ax.set_ylabel(\"Probability\", fontsize=13)\n",
    "ax.set_xlim(0, 6)\n",
    "fig.savefig(\"category_apriori_real_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd02528-29c1-457a-9d4b-57d9f869ce5a",
   "metadata": {},
   "source": [
    "The lift values for The Complete Journey dataset are centered around 1.7, with values raning from ~0.7 to 12, implying that many category choices in the dataset are not independent.\n",
    "\n",
    "Let's take a closer look at category pairs with lift values smaller than 0.8, where the products are substitutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719df00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_rules.loc[:, \"support_sum\"] = (\n",
    "    real_rules.loc[:, \"consequent support\"] + real_rules.loc[:, \"antecedent support\"]\n",
    ")\n",
    "real_rules.query(\"lift < 0.8\").sort_values(\n",
    "    [\"support_sum\", \"lift\"], ascending=[False, False]\n",
    ").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f641da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_category_idx = [112, 110]\n",
    "category_catalog.loc[select_category_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bbd97-38bd-4d85-8012-494d2aaa6cee",
   "metadata": {},
   "source": [
    "Taking the first tuple of categories above, it shows that a customer has less tendency to buy juice and beer at the same\n",
    "time. This seems like a very plausible set of substitutable products since both are beverages.\n",
    "\n",
    "In addition, there appear to be many complementary categories in the Complete Journey dataset. We look at a some category pairs with higher lift values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9145538f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_rules.query(\"lift > 3\").sort_values(\n",
    "    [\"support_sum\", \"lift\"], ascending=[True, True]\n",
    ").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d480a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_category_idx = [116, 221]\n",
    "category_catalog.loc[select_category_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28600218-67fa-4925-8709-f15149043d67",
   "metadata": {},
   "source": [
    "The example reflects a pattern where complementary pet products are commonly purchased at the same time. Again, this seems like a real effect and very unlikely to just be a statistical artifact. To understand the significance of the many large lift values observed in the distribution, we perform the same analysis on the synthetic datataset and compare the results.\n",
    "\n",
    "\n",
    "## Category Choice in Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ad1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare one-hot-encoding vector for each basket\n",
    "synthetic_category_choice = synthetic_result[\"category_choice\"].reset_index()[\n",
    "    [\"category_nbr\", \"week\", \"customer_key\", \"category_choice\"]\n",
    "]\n",
    "synthetic_category_choice = synthetic_category_choice.query(\"category_choice == 1\")\n",
    "synthetic_category_choice = synthetic_category_choice.groupby([\"week\", \"customer_key\"])[\n",
    "    \"category_nbr\"\n",
    "].apply(list)\n",
    "encoder = TransactionEncoder()\n",
    "te_ary = encoder.fit(synthetic_category_choice).transform(synthetic_category_choice)\n",
    "df = pd.DataFrame(te_ary, columns=encoder.columns_)\n",
    "\n",
    "# Building the model\n",
    "frq_items = apriori(df, min_support=0.01, max_len=2, use_colnames=True)\n",
    "\n",
    "# Collecting the inferred rules in a dataframe\n",
    "synthetic_rules = association_rules(frq_items, metric=\"lift\", min_threshold=0)\n",
    "ax = sns.histplot(synthetic_rules.lift, stat=\"probability\")\n",
    "ax.set_title(\"Category-level lift distribution\")\n",
    "ax.set_xlabel(\"Lift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7459bd-69d3-4bc2-8157-7fd74ba4c238",
   "metadata": {},
   "source": [
    "The lift values for the synthetic dataset are centered around 1.06 with standard deviation of 0.17. It shows that when assuming category choices are independent, lift values are typically less than 2. This differs from the Complete Journey dataset where we saw many values >2.\n",
    "\n",
    "High numbers of complementary product categoreis can lead to a fatter tailed choice distribution, since there is an increased probability of buying multiple, complementary products. We discuss further in the paper the significance of this effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
